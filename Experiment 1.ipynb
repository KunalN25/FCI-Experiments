{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "BackPropagation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETdE0S7TElKW"
      },
      "source": [
        "## Kunal Nalawade\n",
        "## 2018130031\n",
        "## Experiment 1\n",
        "### **Aim:**\n",
        "Experiment on Supervised Learning (Back Propagation Neural Network)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAlfgUBGJe7g",
        "outputId": "a16cea2f-b9ec-479a-edfc-8e02ca19bb81"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialisation(n_inputs, n_hidden, n_outputs):\n",
        "       \n",
        "    V = np.array([])\n",
        "    for i in range(n_hidden):\n",
        "        for j in range(n_inputs):\n",
        "            p = float(input('V['+str(i+1)+str(j+1)+']: '))\n",
        "            V = np.append(V,p)\n",
        "    V = V.reshape(n_hidden,n_inputs)\n",
        "    \n",
        "    B1 = np.array([])\n",
        "    for i in range(n_hidden):\n",
        "        p = float(input('B1['+str(i+1)+']: '))\n",
        "        B1 = np.append(B1,p)\n",
        "    B1 = B1.reshape(n_hidden,1)\n",
        "    \n",
        "    W = np.array([])\n",
        "    for i in range(n_outputs):\n",
        "        for j in range(n_hidden):\n",
        "            p = float(input('W['+str(i+1)+str(j+1)+']: '))\n",
        "            W = np.append(W,p)\n",
        "    W = W.reshape(n_outputs,n_hidden)\n",
        "    \n",
        "    B2 = np.array([])\n",
        "    for i in range(n_outputs):\n",
        "        p = float(input('B2['+str(i+1)+']: '))\n",
        "        B2 = np.append(B2,p)\n",
        "    B2 = B2.reshape(n_outputs,1)\n",
        "\n",
        "    return V,B1,W,B2\n",
        "    \n",
        "def forward_propagation(X,V,B1,W,B2):\n",
        "    \n",
        "    z1 = np.dot(V.T, X) + B1\n",
        "    a1 = 1/(1 + np.exp(-z1)) \n",
        "    \n",
        "    z2 = np.dot(W, a1) + B2\n",
        "    Y = 1/(1 + np.exp(-z2)) \n",
        "    \n",
        "    return z1,a1,z2,Y\n",
        "    \n",
        "def cost(Y, y):\n",
        "    \n",
        "    c = (y-Y)*Y*(1-Y)\n",
        "    \n",
        "    return c\n",
        "    \n",
        "def back_propagation(c,V,W,B1,B2,z1,z2,a1,X,alpha):\n",
        "    \n",
        "    delta_W = (np.multiply(alpha*c,a1)).T\n",
        "    delta_B2 = alpha*c\n",
        "    \n",
        "    x = X.T\n",
        "    x = np.concatenate([x,x], axis=0)\n",
        "    delta_V = np.multiply(alpha*x,np.multiply((c*W).T,a1*(1-a1))).T\n",
        "    delta_B1 = alpha*np.multiply((c*W).T,a1*(1-a1))\n",
        "    \n",
        "    new_V = V + delta_V\n",
        "    new_W = W + delta_W\n",
        "    new_B1 = B1 + delta_B1\n",
        "    new_B2 = B2 + delta_B2\n",
        "    \n",
        "    return new_V,new_B1,new_W,new_B2\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    print('Inputs: \\n')\n",
        "    n_inputs = int(input('No of Input Units'))\n",
        "    n_hidden = int(input('No of Hidden Units'))\n",
        "    n_outputs = int(input('No of Output Units'))\n",
        "\n",
        "    X = np.array([])\n",
        "    for i in range(n_inputs):\n",
        "        p = float(input('X'+str(i+1)+': '))\n",
        "        X = np.append(X,p)   \n",
        "    X = X.reshape(n_inputs,1)    \n",
        "\n",
        "    y = int(input('Target Value: '))\n",
        "\n",
        "    V,B1,W,B2 = initialisation(n_inputs,n_hidden,n_outputs)\n",
        "    Y = np.array([0])\n",
        "\n",
        "\n",
        "    z1,a1,z2,Y = forward_propagation(X,V,B1,W,B2)\n",
        "    c = cost(Y, y)\n",
        "    V,B1,W,B2 = back_propagation(c,V,W,B1,B2,z1,z2,a1,X,0.25)\n",
        "    print('\\nOutputs: ')\n",
        "    print('\\nV: ',V)\n",
        "    print('B1: ',B1)\n",
        "    print('W: ',W)\n",
        "    print('B2: ',B2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs: \n",
            "\n",
            "No of Input Units2\n",
            "No of Hidden Units2\n",
            "No of Output Units1\n",
            "X1: 0\n",
            "X2: 1\n",
            "Target Value: 1\n",
            "V[11]: 0.5\n",
            "V[12]: 0.6\n",
            "V[21]: -0.1\n",
            "V[22]: 0.7\n",
            "B1[1]: 0.3\n",
            "B1[2]: 0.4\n",
            "W[11]: 0.3\n",
            "W[12]: 0.3\n",
            "B2[1]: -0.2\n",
            "\n",
            "Outputs: \n",
            "\n",
            "V:  [[ 0.5         0.6       ]\n",
            " [-0.0979182   0.70157592]]\n",
            "B1:  [[0.3020818 ]\n",
            " [0.40157592]]\n",
            "W:  [[0.31541506 0.32103418]]\n",
            "B2:  [[-0.17196415]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Ek1VfqE6Bs"
      },
      "source": [
        "## **Conclusion**\n",
        "1. Backpropagation is done to optimize weights so that neural network can learn\n",
        "to correctly predict the output.\n",
        "2. In a forward propagation, we see how the neural network predicts according\n",
        "to the weights and biases given as input.\n",
        "3. Then we calculate total error for the output of forward propagation.\n",
        "4. After that backward propagation is done to update the weights used above in\n",
        "the neural network.\n",
        "5. By doing the back propagation we reduce the error found out in the forward\n",
        "propagation."
      ]
    }
  ]
}